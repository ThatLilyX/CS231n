{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Loss Curve Visualization\n",
    "\n",
    "In this assignment, you'll learn to visualize and interpret loss curves during neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import all functions from loss_visualization module\nfrom loss_visualization import (\n    SimpleNet,\n    generate_synthetic_data,\n    create_data_loaders,\n    train_with_loss_tracking,\n    plot_loss_curves,\n    compare_learning_rates,\n    identify_training_issues\n)\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Data\n",
    "\n",
    "Create a simple regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate synthetic data using the function from loss_visualization.py\nn_samples = 1000\nn_features = 10\n\nX, y = generate_synthetic_data(n_samples=n_samples, n_features=n_features, noise=0.1)\n\nprint(f\"Data shape: X={X.shape}, y={y.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Train/Val Split and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create train/val data loaders using the function from loss_visualization.py\ntrain_loader, val_loader = create_data_loaders(X, y, train_ratio=0.8, batch_size=32, shuffle=True)\n\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create model using SimpleNet class from loss_visualization.py\nmodel = SimpleNet(input_dim=n_features, hidden_dim=64, output_dim=1)\nprint(model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop with Loss Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train model using the training function from loss_visualization.py\nimport torch.nn as nn\nimport torch.optim as optim\n\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nnum_epochs = 100\nhistory = train_with_loss_tracking(model, train_loader, val_loader, \n                                  criterion, optimizer, num_epochs=num_epochs)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot loss curves using the function from loss_visualization.py\nplot_loss_curves(history, title=\"Training and Validation Loss\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experiment with Different Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare different learning rates using the function from loss_visualization.py\nlearning_rates = [0.001, 0.01, 0.1, 1.0]\n\ncompare_learning_rates(\n    learning_rates=learning_rates,\n    model_fn=lambda: SimpleNet(n_features, 64, 1),\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=100\n)\n\n# Also analyze the training behavior\ndiagnosis = identify_training_issues(history)\nprint(f\"\\nTraining Diagnosis: {diagnosis}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Questions to Answer\n\n### 1. What happens when the learning rate is too high?\n\n**Symptoms:**\n- **Loss diverges or explodes** - The loss values increase dramatically or become NaN (Not a Number)\n- **Unstable training** - Loss oscillates wildly, jumping up and down without converging\n- **Overshooting** - The optimizer \"jumps over\" the optimal solution repeatedly\n- **No learning** - Model fails to learn anything useful\n\n**Why this happens:**\nWhen the learning rate is too high, the weight updates are too large. Imagine trying to find the bottom of a valley while taking giant steps - you'll keep jumping from one side to the other, never settling at the bottom.\n\n**Visual pattern on loss curve:**\n```\nLoss\n ^\n |  /\\    /\\    /\\\n | /  \\  /  \\  /  \\\n |/    \\/    \\/    \\\n +-------------------> Epochs\n   Chaotic, oscillating\n```\n\n---\n\n### 2. What happens when the learning rate is too low?\n\n**Symptoms:**\n- **Very slow convergence** - Loss decreases extremely slowly\n- **Training takes forever** - May require 10x or 100x more epochs to reach good performance\n- **Gets stuck in local minima** - Doesn't have enough \"momentum\" to escape suboptimal solutions\n- **Premature plateau** - Loss stops decreasing before reaching a good solution\n\n**Why this happens:**\nWith a learning rate that's too low, weight updates are tiny. It's like trying to cross a field by taking baby steps - you'll eventually get there, but it takes an extremely long time.\n\n**Visual pattern on loss curve:**\n```\nLoss\n ^\n |___\n |    \\_____\n |          \\________\n |                  \\___\n +-------------------> Epochs\n   Slow, gradual descent\n```\n\n---\n\n### 3. How can you identify overfitting from the loss curves?\n\n**Key indicators:**\n\n**A. The Gap Pattern** (most common):\n- Training loss continues to **decrease**\n- Validation loss **stops decreasing** and starts **increasing**\n- Growing gap between training and validation loss\n\n**Visual pattern:**\n```\nLoss\n ^\n |        Validation Loss\n |       /‾‾‾‾‾‾‾‾‾\n |      /\n |-----/-------------- <- Overfitting starts here\n |    /\n |   /  Training Loss\n |  /    \\_____\n | /           \\_____\n |/                  \\_____\n +-------------------------> Epochs\n```\n\n**B. Other signs:**\n- **Large gap at the end**: `val_loss >> train_loss` (e.g., train=0.1, val=0.8)\n- **Validation loss increasing**: While training loss is still going down\n- **Perfect training performance**: Training loss near zero, but validation loss is high\n\n**What's happening:**\nThe model is memorizing the training data instead of learning generalizable patterns. It's like a student who memorizes answers to practice problems but can't solve new ones.\n\n---\n\n### 4. What techniques could you use to prevent overfitting?\n\nHere are the main techniques, ordered by how commonly they're used:\n\n#### **A. Regularization Techniques**\n\n**1. Dropout** (most popular)\n```python\nself.dropout = nn.Dropout(p=0.5)  # Drop 50% of neurons randomly\n```\n- Randomly \"turns off\" neurons during training\n- Forces the network to learn robust features\n\n**2. Weight Decay (L2 Regularization)**\n```python\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n```\n- Penalizes large weights\n- Encourages simpler models\n\n**3. L1 Regularization**\n- Encourages sparse weights (many weights become exactly zero)\n\n#### **B. Data-Related Techniques**\n\n**4. Get More Training Data**\n- More data = better generalization\n- Use data augmentation if you can't get more real data\n\n**5. Data Augmentation**\n```python\n# For images: random flips, rotations, crops, color jittering\ntransforms.RandomHorizontalFlip()\ntransforms.RandomCrop(32, padding=4)\n```\n- Creates variations of existing data\n- Helps model generalize better\n\n#### **C. Model Architecture Techniques**\n\n**6. Reduce Model Complexity**\n- Use fewer layers or neurons\n- Simpler models are less likely to overfit\n\n**7. Early Stopping**\n```python\n# Stop training when validation loss stops improving\nif val_loss > best_val_loss:\n    patience_counter += 1\nif patience_counter > max_patience:\n    break  # Stop training\n```\n- Monitor validation loss and stop before overfitting gets worse\n\n**8. Batch Normalization**\n```python\nself.bn1 = nn.BatchNorm1d(hidden_dim)\n```\n- Normalizes layer inputs\n- Has a mild regularization effect\n\n#### **D. Training Techniques**\n\n**9. Cross-Validation**\n- Use k-fold cross-validation to ensure model generalizes\n- Helps detect if model only works on one specific validation set\n\n**10. Ensemble Methods**\n- Train multiple models and average their predictions\n- Reduces overfitting through diversity\n\n---\n\n### **Quick Reference Summary**\n\n| Problem | Symptoms | Solution |\n|---------|----------|----------|\n| **LR too high** | Loss explodes, wild oscillations | Decrease learning rate (try 10x smaller) |\n| **LR too low** | Extremely slow training, plateau | Increase learning rate (try 10x larger) |\n| **Overfitting** | Train loss ↓, Val loss ↑, large gap | Dropout, regularization, more data, early stopping |\n| **Underfitting** | Both losses high and not improving | Bigger model, train longer, decrease regularization |\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}