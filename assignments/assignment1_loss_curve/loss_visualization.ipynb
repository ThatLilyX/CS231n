{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Loss Curve Visualization\n",
    "\n",
    "In this assignment, you'll learn to visualize and interpret loss curves during neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import all functions from loss_visualization module\n",
    "from loss_visualization import (\n",
    "    SimpleNet,\n",
    "    generate_synthetic_data,\n",
    "    create_data_loaders,\n",
    "    train_with_loss_tracking,\n",
    "    plot_loss_curves,\n",
    "    compare_learning_rates,\n",
    "    identify_training_issues\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Data\n",
    "\n",
    "Create a simple regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data using the function from loss_visualization.py\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "X, y = generate_synthetic_data(n_samples=n_samples, n_features=n_features, noise=0.1)\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Train/Val Split and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val data loaders using the function from loss_visualization.py\n",
    "train_loader, val_loader = create_data_loaders(X, y, train_ratio=0.8, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model using SimpleNet class from loss_visualization.py\n",
    "model = SimpleNet(input_dim=n_features, hidden_dim=64, output_dim=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop with Loss Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using the training function from loss_visualization.py\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 100\n",
    "history = train_with_loss_tracking(model, train_loader, val_loader, \n",
    "                                  criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves using the function from loss_visualization.py\n",
    "plot_loss_curves(history, title=\"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experiment with Different Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates using the function from loss_visualization.py\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "compare_learning_rates(\n",
    "    learning_rates=learning_rates,\n",
    "    model_fn=lambda: SimpleNet(n_features, 64, 1),\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=100\n",
    ")\n",
    "\n",
    "# Also analyze the training behavior\n",
    "diagnosis = identify_training_issues(history)\n",
    "print(f\"\\nTraining Diagnosis: {diagnosis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to Answer\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What happens when the learning rate is too high?\n",
    "\n",
    "**Symptoms:**\n",
    "- **Loss diverges or explodes** – Loss values increase dramatically or become NaN (Not a Number)\n",
    "- **Unstable training** – Loss oscillates wildly without converging\n",
    "- **Overshooting** – The optimizer repeatedly jumps over the optimal solution\n",
    "- **Failure to converge** – Training does not settle toward a minimum\n",
    "\n",
    "**Why this happens:**\n",
    "When the learning rate is too high, weight updates are excessively large. Imagine trying to find the bottom of a valley while taking giant steps—you keep jumping from one side to the other and never settle at the bottom.\n",
    "\n",
    "**Visual pattern on loss curve:**\n",
    "\n",
    "```\n",
    "Loss\n",
    " ^\n",
    " |  /\\    /\\    /\\\n",
    " | /  \\  /  \\  /  \\\n",
    " |/    \\/    \\/    \\\n",
    " +-------------------> Epochs\n",
    "   Chaotic, oscillating\n",
    "```\n",
    "\n",
    "\n",
    "> Note: This behavior is especially pronounced with plain SGD; adaptive optimizers like Adam or RMSProp can tolerate slightly higher learning rates but will still diverge if the rate is too large.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What happens when the learning rate is too low?\n",
    "\n",
    "**Symptoms:**\n",
    "- **Very slow convergence** – Loss decreases extremely slowly\n",
    "- **Training takes a very long time** – May require 10× or 100× more epochs\n",
    "- **Gets stuck in plateaus or saddle points** – Updates are too small to escape flat regions of the loss surface\n",
    "- **Apparent premature plateau** – Loss appears to stop improving\n",
    "\n",
    "**Why this happens:**\n",
    "With a learning rate that’s too low, weight updates are tiny. It’s like trying to cross a field by taking baby steps—you’ll eventually move forward, but progress is painfully slow.\n",
    "\n",
    "**Visual pattern on loss curve:**\n",
    "\n",
    "```\n",
    "Loss\n",
    " ^\n",
    " |___\n",
    " |    \\_____\n",
    " |          \\________\n",
    " |                  \\___\n",
    " +-------------------> Epochs\n",
    "   Slow, gradual descent\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How can you identify overfitting from the loss curves?\n",
    "\n",
    "**Key indicators:**\n",
    "\n",
    "**A. The Gap Pattern** (most common):\n",
    "- Training loss continues to **decrease**\n",
    "- Validation loss **stops decreasing** and starts **increasing** or plateauing\n",
    "- Growing gap between training and validation loss\n",
    "\n",
    "**Visual pattern:**\n",
    "```\n",
    "Loss\n",
    " ^\n",
    " |        Validation Loss\n",
    " |       /‾‾‾‾‾‾‾‾‾\n",
    " |      /\n",
    " |-----/-------------- <- Overfitting starts here\n",
    " |    /\n",
    " |   /  Training Loss\n",
    " |  /    \\_____\n",
    " | /           \\_____\n",
    " |/                  \\_____\n",
    " +-------------------------> Epochs\n",
    "```\n",
    "\n",
    "**B. Other signs:**\n",
    "- **Large generalization gap**: `val_loss >> train_loss` (e.g., train=0.1, val=0.8)\n",
    "- **Validation loss stagnates or increases**: While training loss is still going down\n",
    "- **Perfect training performance** with poor validation performance\n",
    "\n",
    "**What's happening:**\n",
    "The model is memorizing the training data instead of learning generalizable patterns. It's like a student who memorizes answers to practice problems but can't solve new ones.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What techniques could you use to prevent overfitting?\n",
    "\n",
    "Here are the main techniques, ordered by how commonly they're used:\n",
    "\n",
    "#### **A. Regularization Techniques**\n",
    "\n",
    "**1. Dropout** (most popular)\n",
    "```python\n",
    "self.dropout = nn.Dropout(p=0.5)  # Drop 50% of neurons randomly\n",
    "```\n",
    "- Randomly \"turns off\" neurons during training\n",
    "- Forces the network to learn robust features\n",
    "- Applied only during training, not inference\n",
    "\n",
    "**2. Weight Decay (L2 Regularization)**\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "```\n",
    "- Penalizes large weights\n",
    "- Encourages simpler models\n",
    "\n",
    "**3. L1 Regularization**\n",
    "- Encourages sparse weights (many weights become exactly zero)\n",
    "- More common in linear models; less used in deep networks due to optimization difficulty\n",
    "\n",
    "#### **B. Data-Related Techniques**\n",
    "\n",
    "**4. Get More Training Data**\n",
    "- More data = better generalization\n",
    "- Use data augmentation if you can't get more real data\n",
    "\n",
    "**5. Data Augmentation**\n",
    "```python\n",
    "# For images: random flips, rotations, crops, color jittering\n",
    "transforms.RandomHorizontalFlip()\n",
    "transforms.RandomCrop(32, padding=4)\n",
    "```\n",
    "- Creates realistics variations of existing data\n",
    "- Often the most effective regularization method in computer vision\n",
    "\n",
    "#### **C. Model Architecture Techniques**\n",
    "\n",
    "**6. Reduce Model Complexity**\n",
    "- Use fewer layers or neurons\n",
    "- Simpler models are less likely to overfit\n",
    "\n",
    "**7. Early Stopping**\n",
    "```python\n",
    "# Stop training when validation loss stops improving\n",
    "if val_loss > best_val_loss:\n",
    "    patience_counter += 1\n",
    "if patience_counter > max_patience:\n",
    "    break  # Stop training\n",
    "```\n",
    "- Monitor validation loss and stop before overfitting gets worse\n",
    "\n",
    "**8. Batch Normalization**\n",
    "```python\n",
    "self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "```\n",
    "- Stabilizes and accelerates training\n",
    "- Enables higher learning rates\n",
    "- Only has a **mild** regularization effect\n",
    "\n",
    "#### **D. Training Techniques**\n",
    "\n",
    "**9. Cross-Validation**\n",
    "- Use k-fold cross-validation to ensure model generalizes\n",
    "- Helps detect if model only works on one specific validation set\n",
    "- Common in classical ML and small datasets; Rarely used in large-scale deep learning due to computational cost\n",
    "\n",
    "**10. Ensemble Methods**\n",
    "- Train multiple models and average their predictions\n",
    "- Reduces overfitting through diversity; Reduce variance and improve generalization\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick Reference Summary**\n",
    "\n",
    "| Problem | Symptoms | Solution |\n",
    "|---------|----------|----------|\n",
    "| **LR too high** | Loss explodes, wild oscillations | Decrease learning rate (try 10x smaller) |\n",
    "| **LR too low** |  Slow training, plateau | Increase learning rate (try 10x larger) |\n",
    "| **Overfitting** | Train loss ↓, Val loss ↑, large gap | Dropout, regularization, more data, augmentation, early stopping |\n",
    "| **Underfitting** | Both losses high and not improving | Bigger model, better features, train longer, decrease regularization, adjust LR |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
