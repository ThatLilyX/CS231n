{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: 2-Layer MLP From Scratch\n",
    "\n",
    "**Goal**: Implement a 2-layer Multi-Layer Perceptron without using `nn.Sequential`.\n",
    "\n",
    "## Requirements\n",
    "- ❌ No `nn.Sequential` allowed\n",
    "- ✅ Use `nn.Linear`, `nn.ReLU` but define them explicitly\n",
    "- ✅ Implement forward pass manually\n",
    "- ✅ Achieve >85% accuracy on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Implement the 2-Layer MLP\n",
    "\n",
    "**Important**: Do NOT use `nn.Sequential`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer MLP implemented from scratch.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (784) -> Linear -> ReLU -> Linear -> Output (10)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerMLP, self).__init__()\n",
    "        \n",
    "        # TODO: Define layers (NO nn.Sequential!)\n",
    "        self.fc1 = None  # First linear layer\n",
    "        self.relu = None  # Activation function\n",
    "        self.fc2 = None  # Second linear layer\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n",
    "        # TODO: Use nn.init.xavier_uniform_ or kaiming_uniform_\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 28, 28) or (batch_size, 784)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 10)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # 1. Flatten input: x.view(x.size(0), -1)\n",
    "        # 2. First layer + activation\n",
    "        # 3. Second layer\n",
    "        # 4. Return output (no softmax - CrossEntropyLoss handles it)\n",
    "        \n",
    "        pass\n",
    "\n",
    "# Create model instance\n",
    "model = TwoLayerMLP(input_dim=28*28, hidden_dim=256, output_dim=10).to(device)\n",
    "\n",
    "# Verify no nn.Sequential is used\n",
    "has_sequential = any(isinstance(m, nn.Sequential) for m in model.modules())\n",
    "print(f\"Uses nn.Sequential: {has_sequential} (should be False)\")\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize a batch of images\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i in range(10):\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(example_data[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {example_targets[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "print(\"Loss function:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement training loop\n",
    "num_epochs = 10\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Your training code here\n",
    "        pass\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Your validation code here\n",
    "            pass\n",
    "    \n",
    "    # Record history and print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot loss and accuracy curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "epochs = range(1, num_epochs + 1)\n",
    "ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
    "ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(epochs, history['train_acc'], 'b-', label='Train Acc')\n",
    "ax2.plot(epochs, history['val_acc'], 'r-', label='Val Acc')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    examples = enumerate(test_loader)\n",
    "    batch_idx, (example_data, example_targets) = next(examples)\n",
    "    example_data = example_data.to(device)\n",
    "    \n",
    "    output = model(example_data)\n",
    "    predictions = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i in range(10):\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(example_data[i].cpu().squeeze(), cmap='gray')\n",
    "    pred = predictions[i].item()\n",
    "    true = example_targets[i].item()\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'Pred: {pred}, True: {true}', color=color)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### 1. What is the advantage of NOT using `nn.Sequential`?\n",
    "\n",
    "**Answer:**  \n",
    "The main advantage is **greater flexibility and control over the forward pass**.\n",
    "\n",
    "- Enables complex architectures (skip connections, multi-branch networks)\n",
    "- Allows conditional logic inside `forward()`\n",
    "- Supports layer reuse and weight sharing\n",
    "- Makes it easier to inspect or manipulate intermediate activations\n",
    "- Allows insertion of custom operations (attention, gating, etc.)\n",
    "\n",
    "`nn.Sequential` is best for simple linear stacks of layers, while custom modules allow full computational graph control.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How does weight initialization affect training?\n",
    "\n",
    "**Answer:**  \n",
    "Weight initialization affects **training stability, convergence speed, and final performance**.\n",
    "\n",
    "- **Too small weights** → vanishing activations and gradients\n",
    "- **Too large weights** → exploding activations and gradients\n",
    "- Proper initialization preserves activation and gradient variance across layers\n",
    "\n",
    "**Common strategies:**\n",
    "- **Xavier (Glorot)** initialization for tanh/sigmoid activations\n",
    "- **He (Kaiming)** initialization for ReLU-based activations\n",
    "\n",
    "Good initialization helps gradients flow efficiently, especially in deep networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What happens if you use too many / too few hidden units?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **Too few hidden units**\n",
    "  - Underfitting\n",
    "  - High bias\n",
    "  - Model cannot capture data complexity\n",
    "  - Poor training and test performance\n",
    "\n",
    "- **Too many hidden units**\n",
    "  - Overfitting\n",
    "  - High variance\n",
    "  - Good training performance, poor generalization\n",
    "  - Increased computation and memory cost\n",
    "\n",
    "The number of hidden units should match the complexity of the data and be controlled with regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. How could you improve the model’s accuracy?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- **Architecture**\n",
    "  - Add or adjust hidden layers/units\n",
    "  - Use better activation functions\n",
    "  - Add residual connections\n",
    "\n",
    "- **Optimization**\n",
    "  - Tune learning rate\n",
    "  - Use better optimizers (Adam, AdamW)\n",
    "  - Apply learning rate schedules\n",
    "\n",
    "- **Regularization**\n",
    "  - Dropout\n",
    "  - Weight decay (L2)\n",
    "  - Batch normalization\n",
    "  - Early stopping\n",
    "\n",
    "- **Data**\n",
    "  - Collect more data\n",
    "  - Apply data augmentation\n",
    "  - Improve feature quality\n",
    "\n",
    "- **Training**\n",
    "  - Better weight initialization\n",
    "  - Train longer if not overfitting\n",
    "\n",
    "Improving accuracy requires balancing model capacity, optimization, regularization, and data quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
